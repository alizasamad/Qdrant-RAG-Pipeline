,metric,split_char_512_50_mean,split_char_2048_200_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.33999999996599994,0.519999999948,-0.179999999982,False,True,0.005100014825958168,True,-0.279999999972,-0.079999999992,0.6421554612114082
1,context_recall_score,0.3188015873015873,0.5043888888888888,-0.1855873015873016,False,True,0.0003104803478910181,True,-0.28236409218952196,-0.09571038076938475,0.7124777659070367
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.39849433731279027,0.43156002874384536,-0.033065691431055096,False,True,0.17490773962955294,False,,,0.24813152956851905
4,answer_relevancy_score,0.5996880939119118,0.6432003765132174,-0.043512282601305644,False,True,0.4043908712939438,False,,,0.16541663960037933
5,answer_faithfulness_score,0.863184149184149,0.8436385901680019,0.019545559016147233,True,False,0.3951107708913837,False,,,-0.17255281059871094
6,rag_latency,4.994047673543295,5.874867897033693,-0.8808202234903969,True,False,1.0715179121042549e-09,True,-1.1141084979587403,-0.659008842836299,1.4274724075238994
