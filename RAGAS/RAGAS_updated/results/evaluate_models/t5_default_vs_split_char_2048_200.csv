,metric,split_char_2048_200_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.519999999948,0.3666666666299999,0.15333333331799998,True,False,0.007286310327327169,True,0.059999999994,0.239999999976,-0.5264559247309399
1,context_recall_score,0.5043888888888888,0.3568253968253968,0.14756349206349206,True,False,0.002432034014311561,True,0.062062321438399085,0.24010725532504987,-0.5248503119761753
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.43156002874384536,0.39147427660802064,0.04008575213582464,True,False,0.0838638038358659,False,,,-0.26867596222019735
4,answer_relevancy_score,0.6432003765132174,0.5851945253259734,0.05800585118724402,True,False,0.260516991983128,False,,,-0.21680488398659495
5,answer_faithfulness_score,0.8436385901680019,0.8671857439945676,-0.02354715382656557,False,True,0.30267401309203673,False,,,0.20703185294668652
6,rag_latency,5.874867897033693,5.44855150381724,0.426316393216451,False,True,0.0010951813662066598,True,0.2018859326482178,0.6804609957607289,-0.6510140205515742
