# Introduction
Welcome to experimenting with RAGAS! The goal of this section is to compare RAG model performance metrics on Human Capital data to determine whether certain parameter combinations perform better than others.

We generated a synthetic RAG dataset for HC data using 3 different methods: naive, [AWS](https://aws.amazon.com/blogs/machine-learning/generate-synthetic-data-for-evaluating-rag-systems-using-amazon-bedrock/#:~:text=Load%20the%20data%20from%20your,or%20LLMs%20using%20critique%20agents.), and [RAGAS](https://docs.ragas.io/en/stable/howtos/customizations/testgenerator/_testgen-customisation/). We then used [AWS's LLM-as-an-evaluator protocol](https://github.com/aws-samples/generating-synthetic-datasets-for-evaluating-retrieval-augmented-generation-systems/tree/main) to evaluate the relevancy and groundedness of the generated questions. 

After determining the optimal dataset generation method, we ran our RAGAS metrics evaluation script for each model on three unique datasets. The datasets include two HC-related sets and one control set containing a subset of Google's Natural Query dataset. For each model, we conducted an ANOVA on model performance between all three datasets. This was done to determine whether our RAG model performs significantly better on HC data specifically.

Finally, we conducted paired T-tests to determine which factors, or combinations of factors, significantly impacted RAG performance on a statistical level.

# Method
If you haven't already, please run `pip install -r requirements.txt`.

## Step 1: Generating Synthetic Datasets (optional)
This step is only required if you do not have sufficient RAG datasets already available! Do not run if not necessary to prevent excessive costs.

1. Navigate to the `test_data_synth` directory
2. Generate synthetic datasets. 
    1. You must run at least 'Step 1' of `aws_dataset_synth.ipynb` before generating datasets by any of the three methods.
    2. Run `aws_dataset_synth.ipynb`, `ragas_dataset_synth.py`, and `naive_dataset_synth.py`. Save results in the `results` folder.
    3. Note the adjustable parameters at the beginning of each of the three scripts. These are the only parameters that MUST be adjusted before each run.
3. Evaluate resultant datasets with `test_dataset_synth.ipynb`. Adjust parameters as needed.
4. Run `synth_summary_stats.py` to generate summary statistics for all three datasets. Adjust parameters as needed.
    1. Compare results & conclude what the best synthesis method is. Metrics displayed in terminal; visuals stored in `dataviz` folder.
    2. Use only sets generated by this method during the next step.

## Step 2: Running RAGAS Metrics Tests
The very first model will be OWUI's default RAG settings. This will serve as a benchmark for measuring further model performance metrics.

1. Navigate to the `RAGAS` working directory if not already there.
2. Run `ragas_metrics.py` using perferred RAG model and collection.
    1. Adjust parameters as necessary.
    2. Results will be stored in the `experiments` folder.
3. Run `ragas_summary_stats.py`. 
    1. Adjust parameters as needed.
    2. Output csv file and png plots will be available in the `results/summary_stats` folder.
4. Repeat steps 1-3 on all datasets ('google_NQ', 'swa', 't5') for each model.
5. If `compare` is True, bar chart comparing metrics from all files listed will be created in the `summary_stats` folder. 

## Step 3: Evaluating Model Performance
This section will evaluate model performance horizontally (one model's performance between several datasets) and vertically (several models' performances against one dataset / the average of multiple datasets). Horizontal analysis is conducted to determine whether RAG can be specifically optimized for Human Capital data queries. Vertical analysis is conducted to determine which parameters generally enhance RAG performance.

### Getting Set Up
1. `clean_experiment.py` in the `RAGAS` working directory.
2. Run script as per instructed. Make sure to include all datasets from the `experiments` folder that you would like to clean & evaluate.
    1. Note: All datasets must be cleaned before proceeding through any further analysis steps.
3. You may now proceed to run `evaluate_models.py` as defined below.

### Vertical Analysis:
1. Run section 1 of `evaluate_models.py` as instructed.
2. Results will be displayed in terminal & saved as a CSV in `results/evaluate_models`.

### Horizontal Analysis:
1. Run section 2 of `evaluate_models.py` as instructed.
2. Results will be displayed in terminal & saved as a CSV in `results/evaluate_models`.

### Final Score:
1. Run section 3 of `evaluate_models.py` as instructed.
2. Results will be displayed in terminal & saved as a PNG in `results/evaluate_models`.


