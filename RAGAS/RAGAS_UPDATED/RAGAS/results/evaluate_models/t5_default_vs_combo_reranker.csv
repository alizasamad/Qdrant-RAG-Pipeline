,metric,combo_reranker_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.8399999999159999,0.3666666666299999,0.4733333332860001,True,False,4.9394807514071914e-08,True,0.35333333329800004,0.5799999999419999,-1.398890429784082
1,context_recall_score,0.8111666666666667,0.3568253968253968,0.4543412698412699,True,False,1.552667979488785e-07,True,0.34205803621958997,0.5604602141686176,-1.3681208630694481
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.5477748759898293,0.39147427660802064,0.15630059938180854,True,False,2.1304459965370697e-06,True,0.10265464605315949,0.21400890594549635,-0.8127128729781797
4,answer_relevancy_score,0.8248408083338324,0.5851945253259734,0.239646283007859,True,False,1.444953007303496e-06,True,0.153877177939353,0.32440055121812733,-0.8719279333445868
5,answer_faithfulness_score,0.9285750915750915,0.8671857439945676,0.061389347580524056,True,False,0.0022986682481677626,True,-0.013790220671713,0.10712551798213478,-0.4398494830495204
6,rag_latency,59.66640913009643,5.44855150381724,54.21785762627919,False,True,1.0438241429663671e-46,True,52.51759235597533,56.15148461262601,-11.573423258058217
