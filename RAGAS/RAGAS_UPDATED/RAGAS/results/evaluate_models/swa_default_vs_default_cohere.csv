,metric,default_cohere_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.479999999952,0.3999999999599999,0.07999999999199998,True,False,0.09425293454417466,False,,,-0.2793798441998508
1,context_recall_score,0.4472222222222223,0.3837777777777777,0.06344444444444447,True,False,0.15706850446984913,False,,,-0.24096455165869995
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.361067191519672,0.3293269266534617,0.03174026486621022,True,False,0.14465712182059348,False,,,-0.2483353864719758
4,answer_relevancy_score,0.5970272379295714,0.6389068910734071,-0.04187965314383582,False,True,0.28924384526096664,False,,,0.18624941059089298
5,answer_faithfulness_score,0.8516344359344358,0.8362501825659721,0.01538425336846387,True,False,0.48327338375126627,False,,,-0.12976382546334547
6,rag_latency,5.935392980575561,5.417910820643106,0.5174821599324545,False,True,0.0007425823793309583,True,0.2287568584586319,0.7810574631392295,-0.7773543225855862
