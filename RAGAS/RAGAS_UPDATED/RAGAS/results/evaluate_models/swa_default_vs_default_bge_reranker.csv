,metric,default_bge-reranker_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.593333333274,0.3999999999599999,0.193333333314,True,False,0.003504808682011228,True,0.079999999992,0.286666666638,-0.6834397026413527
1,context_recall_score,0.567,0.3837777777777777,0.1832222222222222,True,False,0.0010819242401244455,True,0.08066666666666668,0.2840979242131685,-0.7147120092589327
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.4072520202119479,0.3293269266534617,0.07792509355848624,True,False,0.0027031799306614533,True,0.030150206806865837,0.12577744873663202,-0.5357904226390104
4,answer_relevancy_score,0.7730424069822268,0.6389068910734071,0.13413551590881975,True,False,0.0029132653173264,True,0.055475721166484915,0.2221998790943452,-0.6308409082946236
5,answer_faithfulness_score,0.8126253436106378,0.8362501825659721,-0.023624838955334314,False,True,0.27938509239848125,False,,,0.18330078586717222
6,rag_latency,13.01619158109029,5.417910820643106,7.598280760447184,False,True,1.7763568394002505e-15,True,7.089388499928517,8.055763480495605,-6.063863450182187
