,metric,combo_reranker_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.795918367267347,0.4081632652653061,0.38775510200204083,True,False,1.6454483915856003e-06,True,0.265306122422449,0.4897959183183675,-1.46942929080386
1,context_recall_score,0.7054421768707482,0.391609977324263,0.3138321995464853,True,False,7.719988791635852e-07,True,0.20498866213151928,0.4174088517525517,-1.2113750490732293
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.4355434598775188,0.3333141670534247,0.10222929282409399,True,False,0.0006376842479310231,True,0.05090900018001982,0.15844477818389305,-0.7303870173861685
4,answer_relevancy_score,0.8597617016535308,0.6391913160315414,0.2205703856219895,True,False,5.275880221209061e-07,True,0.14945427589746094,0.29852643275968155,-1.12005294004851
5,answer_faithfulness_score,0.8446475893638971,0.8341452079368299,0.010502381427067092,True,False,0.6952450583473333,False,,,-0.08369544850753484
6,rag_latency,48.12748433943508,5.363127843052351,42.76435649638274,False,True,3.552713678800501e-15,True,40.77236551322471,47.08462430222674,-6.0131607956983455
