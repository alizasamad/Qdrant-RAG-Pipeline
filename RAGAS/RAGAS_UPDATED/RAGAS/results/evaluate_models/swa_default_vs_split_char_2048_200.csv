,metric,split_char_2048_200_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.393333333294,0.3999999999599999,-0.006666666665999998,False,True,0.973187818767502,False,,,0.025271416850114168
1,context_recall_score,0.36666666666666664,0.3837777777777777,-0.017111111111111098,False,True,0.7250969460487693,False,,,0.06564242979434935
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.30954218866024974,0.3293269266534617,-0.019784737993211926,False,True,0.27061811066382235,False,,,0.16289363792916736
4,answer_relevancy_score,0.6000882654754444,0.6389068910734071,-0.038818625597962765,False,True,0.38913591510383116,False,,,0.15599567584531515
5,answer_faithfulness_score,0.8675990110107756,0.8362501825659721,0.031348828444803675,True,False,0.16862425680023288,False,,,-0.25531825543984776
6,rag_latency,5.6694397989908865,5.417910820643106,0.2515289783477784,False,True,0.05022609827180665,False,,,-0.34749296600165847
