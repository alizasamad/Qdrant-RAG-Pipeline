,metric,split_char_512_50_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.31999999996799994,0.3999999999599999,-0.07999999999200001,False,True,0.20782480418103244,False,,,0.2801556853053293
1,context_recall_score,0.27888888888888885,0.3837777777777777,-0.10488888888888886,False,True,0.034193562489386105,True,-0.19630519499561255,-0.011405099262204902,0.4159537667339074
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.32598915545680446,0.3293269266534617,-0.0033377711966571656,False,True,0.8778649881691754,False,,,0.026512071179032877
4,answer_relevancy_score,0.5418870919236847,0.6389068910734071,-0.09701979914972256,False,True,0.0536019514200997,False,,,0.38867023440163534
5,answer_faithfulness_score,0.7935147075147074,0.8362501825659721,-0.04273547505126454,False,True,0.09619100809470489,False,,,0.34130969293051755
6,rag_latency,5.1673305177688595,5.417910820643106,-0.25058030287424715,True,False,0.05380814453987253,False,,,0.38432438373195943
