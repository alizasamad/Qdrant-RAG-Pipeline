,metric,splitter_markdown_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.393333333294,0.3999999999599999,-0.006666666665999998,False,True,0.9452743498121585,False,,,0.02376981960412549
1,context_recall_score,0.32666666666666666,0.3837777777777777,-0.057111111111111106,False,True,0.24298311181050147,False,,,0.2153710806898732
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.32399874584460275,0.3293269266534617,-0.005328180808858822,False,True,0.8172255863904848,False,,,0.040267243179068095
4,answer_relevancy_score,0.5837508141182558,0.6389068910734071,-0.05515607695515132,False,True,0.20954839534902825,False,,,0.21904046573207367
5,answer_faithfulness_score,0.8399669404669404,0.8362501825659721,0.0037167579009684194,True,False,0.8742115686946503,False,,,-0.028641556993409492
6,rag_latency,5.419950019518535,5.417910820643106,0.0020391988754272995,False,True,0.9873786607245087,False,,,-0.003009724848273874
