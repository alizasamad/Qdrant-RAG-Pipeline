,metric,top_k_10_mean,top_k_5_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.653333333268,0.4999999999499999,0.15333333331799998,True,False,0.005146614188281132,True,0.053333333328,0.23333333331000006,-0.5505279016330321
1,context_recall_score,0.588111111111111,0.4595555555555556,0.12855555555555556,True,False,0.004037701246873814,True,0.04511111111111111,0.21166666666666667,-0.49284826063972703
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.37217485291272034,0.3442080489719281,0.027966803940792238,True,False,0.1771556670149414,False,,,-0.20292728960277062
4,answer_relevancy_score,0.8184956518876995,0.7203318812467807,0.0981637706409185,True,False,0.01574343835962111,True,0.031141908944811694,0.176266691926652,-0.49219145337476766
5,answer_faithfulness_score,0.7955316453698806,0.7966022670140316,-0.0010706216441510553,False,True,0.9675864796592973,False,,,0.0064497735537170295
6,rag_latency,6.59897935072581,5.688420311609904,0.9105590391159057,False,True,2.114853944186022e-07,True,0.6212027184084342,1.203407111779607,-1.0946310596906847
