,metric,default_cohere_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.7291666665937501,0.6944444443750001,0.034722222218750015,True,False,0.46566828903627366,False,,,-0.121600041197066
1,context_recall_score,0.6527777777777778,0.6319444444444444,0.02083333333333333,True,False,0.8589955729079024,False,,,-0.07327049431102818
2,non_LLM_context_recall_score,0.006944444444444444,0.027777777777777776,-0.020833333333333332,False,True,0.17971249487899976,False,,,0.28113319697158123
3,rouge_score,0.1997334693761584,0.18780722769923933,0.011926241676919117,True,False,0.43936162799266054,False,,,-0.1308019482186967
4,answer_relevancy_score,0.5547561926518583,0.6923802451453006,-0.13762405249344242,False,True,2.1455032542909154e-05,True,-0.19321459420943124,-0.08001445878725297,0.8203929950214577
5,answer_faithfulness_score,0.783912037037037,0.8686342592592592,-0.08472222222222224,False,True,0.0019943162288206208,True,-0.12824074074074077,-0.03310185185185187,0.5985684633360098
6,rag_latency,4.138040903541778,3.7947683268123207,0.3432725767294566,False,True,1.6643435035552428e-07,True,0.23895513492378,0.451117215784299,-1.0462497903605528
