,metric,bge-reranker_BM25_75_mean,default_mean,mean_diff,improved,regressed,p_value,significant,CI_95_low,CI_95_high,effect_size
0,context_precision_score,0.6933333332639999,0.3999999999599999,0.293333333304,True,False,7.520942512552876e-06,True,0.19333333331400002,0.393333333294,-1.0749274116500551
1,context_recall_score,0.649,0.3837777777777777,0.26522222222222225,True,False,9.310053833589073e-08,True,0.18557810613082823,0.34766666666666673,-1.0519706029874236
2,non_LLM_context_recall_score,0.0,0.0,0.0,False,False,,False,,,
3,rouge_score,0.4437261234797142,0.3293269266534617,0.11439919682625252,True,False,4.84979258743625e-05,True,0.06506998253040033,0.16515901323158658,-0.79034174494381
4,answer_relevancy_score,0.824081295802638,0.6389068910734071,0.18517440472923086,True,False,3.801371960321376e-06,True,0.12292499272362042,0.25991507875347053,-0.8882542860436594
5,answer_faithfulness_score,0.8579439911939911,0.8362501825659721,0.021693808628019156,True,False,0.2935589076891962,False,,,-0.18356829874073347
6,rag_latency,18.266044344902042,5.417910820643106,12.848133524258934,False,True,2.3233737167551018e-48,True,12.450641839762874,13.246835542114693,-11.025489366040578
