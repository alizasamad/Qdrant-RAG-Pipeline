,metric,mean,std_btwn_samples,mean_std_wtn_sample,ci_95_lower,ci_95_upper,cv_btwn_samples,cv_wtn_sample
0,rag_latency,0.5189984054565426,0.24159676403440114,0.09864567023017747,0.44884601788365563,0.5891507930294296,0.46550579249251817,0.19006931272438635
1,context_precision_score,0.6944444443750001,0.2530675993953682,0.31426968049592746,0.6209612734149947,0.7679276153350054,0.3644173431657719,0.4525483399593903
2,context_recall_score,0.6319444444444444,0.2741956530888838,0.3339115355603141,0.5523263262003056,0.7115625626885832,0.4338920224703216,0.5283874848426948
3,non_LLM_context_recall_score,0.027777777777777776,0.0921284663987611,0.039283710065919304,0.0010264596997702873,0.05452909585578526,3.3166247903554,1.414213562373095
4,rouge_score,0.1878072276992393,0.08680932686736874,0.09857105087925437,0.16260042674029127,0.21301402865818733,0.4622256977584913,0.5248522758512219
5,answer_relevancy_score,0.6923802451453005,0.1579142363543862,0.22802593810729624,0.6465267306899829,0.7382337596006181,0.22807443953177328,0.3293362855253669
6,answer_faithfulness_score,0.8686342592592594,0.14035105156014963,0.13909366641719237,0.8278805620619415,0.9093879564565773,0.16157669360155796,0.16012915094530872
